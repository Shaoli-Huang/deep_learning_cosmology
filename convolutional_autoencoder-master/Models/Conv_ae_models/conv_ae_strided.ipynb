{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New .py for pure lasagne\n",
    "# This is Convolutional Autoencoder using Theano and Lasagne wrapper. \n",
    "\n",
    "import os \n",
    "import sys \n",
    "import h5py \n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "sys.path.insert(0,'/global/common/cori/software/theano/0.8.2/lib/python2.7/site-packages/')\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "sys.path.insert(0,'/global/common/cori/software/lasagne/0.1/lib/python2.7/site-packages/')\n",
    "import lasagne\n",
    "\n",
    "from lasagne.layers import InputLayer, DenseLayer\n",
    "from lasagne.layers import Conv2DLayer, Deconv2DLayer\n",
    "from lasagne.layers import MaxPool2DLayer \n",
    "from lasagne.layers import ReshapeLayer \n",
    "\n",
    "from lasagne.objectives import squared_error\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# ################## Load the dataset ################################\n",
    "# Here the dataset is Cosmological maps with Image size 128x128 with total \n",
    "# sample size 64,000. The raw training data is unpacked from a h5py file \n",
    "# The data is then split into Training and Validation sets. Remember this\n",
    "# is Unsupervised Learning so there are no Labels. \n",
    "\n",
    "def load_data():\n",
    "\n",
    "    # Load the dataset\n",
    "    \n",
    "    # Here we give different data sets for the autoencoders\n",
    "    dataurl = '/global/homes/s/ssingh79/data/'\n",
    "    #hdf5file = 'conv_z02.h5'\n",
    "    hdf5file = 'train_data_64k.h5'\n",
    "    filepath = os.path.join(dataurl, hdf5file)\n",
    "    \n",
    "    print(\"Calling \", hdf5file, \"......\")\n",
    "    # Call the load_data method to get back the Final training set. \n",
    "    dataset = filepath\n",
    "    \n",
    "    # Change this to create the Sample size!!!\n",
    "    sample_size = 100\n",
    "    \n",
    "    with h5py.File(dataset,'r') as hf:\n",
    "        #train_set = hf['X_train'][0:1000,0:65536]\n",
    "        train_set = hf['X_train'][0:sample_size,:]\n",
    "        print(\"Printing Train set \", train_set)\n",
    "        print(\"X_train shape \", train_set.shape)\n",
    "   \n",
    "    #Specify the training set percent. \n",
    "    split_percent = 0.90\n",
    "\n",
    "    #################### Preprocessing ###############################\n",
    "    # We first pass the train_set to reshape_data(), then training data is split into training set and validation set & then both the data sets are normalized. The reshape funtion then reshapes the falttened images into 2D images of (128,128) which is our final dataset that goes as input to the Network! :)\n",
    "    \n",
    "    def train_valid_split(train_set, sample_size, split_percent):\n",
    "        #Create Training set and Validation set: Randomly Sampling the images by %. \n",
    "        X = np.random.choice(sample_size, sample_size, replace=False)\n",
    "          \n",
    "        #Get the random indices of images.  \n",
    "        train_split = sample_size*split_percent\n",
    "        train_index = X[0:train_split]\n",
    "        valid_index = X[train_split:sample_size]\n",
    "\n",
    "        # Training set\n",
    "        train_x = train_set[train_index[:], : ]\n",
    "        print(\"Training Set : \", train_x)\n",
    "        print(train_x.shape) \n",
    "\n",
    "        #Validation set\n",
    "        valid_x = train_set[valid_index[:], : ]\n",
    "        print(\"Validation Set : \", valid_x)\n",
    "        print(valid_x.shape)\n",
    "        \n",
    "        visualize_raw_data(train_x)\n",
    "        #visualize_raw_data(valid_x)\n",
    "        \n",
    "        return train_x, valid_x \n",
    "    \n",
    "    def normalize(train_x, valid_x):\n",
    "        # Normalization of dataset goes here : \n",
    "        X_norm = np.linalg.norm(train_x, axis=0)\n",
    "        train_x = train_x/X_norm[np.newaxis,:]\n",
    "        valid_x = valid_x/X_norm[np.newaxis,:]\n",
    "        print(\"Normalized Training data : \", train_x)\n",
    "        print(\"Mean : \", np.mean( train_x))\n",
    "        print(\"min : \", np.min(train_x))\n",
    "        print(\"max : \", np.max(train_x))\n",
    "        print(\"Valid set min : \", np.min(valid_x))\n",
    "        print(\"Valid set max : \", np.max(valid_x))\n",
    "        \n",
    "        return train_x, valid_x \n",
    "    \n",
    "    def reshape_data(train_set):\n",
    "        # Call the train_valid_split creating random samples. \n",
    "        train_x, valid_x = train_valid_split(train_set, sample_size, split_percent)\n",
    "        \n",
    "        # Normalize the training data and validation data. \n",
    "        train_x, valid_x = normalize(train_x, valid_x) \n",
    "        \n",
    "        # Here goes the code to Reshape to 2D images. Return the 2D images as \n",
    "        # X_train and x_validation after reshaping. \n",
    "        train_x = train_x.reshape(-1,1,128,128)\n",
    "        print(\"After Reshaping, training data : \", train_x.shape)\n",
    "        valid_x = valid_x.reshape(-1,1,128,128)\n",
    "        print(\"After Reshaping, validation data : \", valid_x.shape)\n",
    "        \n",
    "        # Visualize the normlaized data. \n",
    "        visualize_normalized_data(train_x)\n",
    "        \n",
    "        return train_x, valid_x\n",
    "    \n",
    "    def visualize_raw_data(train_x):\n",
    "        # Save image as png to Output_files folder. \n",
    "        image = np.reshape(train_x[0,:],(128,128))\n",
    "        print(\"Saving Training image .............\")\n",
    "        plt.imsave('/global/homes/s/ssingh79/Output_files/training_x1.png', image)\n",
    "        \n",
    "        # Save histogram of pixels in an image. \n",
    "        fig = plt.figure(figsize=(5, 4))\n",
    "        ax = fig.add_subplot(1,1,1) # one row, one column, first plot\n",
    "        ax.hist((image), bins=20)\n",
    "        ax.set_title(\"Raw image pixels Histogram Plot\")\n",
    "        ax.set_xlabel(\"Pixel values\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        print(\"Saving histogram of raw image.....\")\n",
    "        fig.savefig(\"/global/homes/s/ssingh79/Output_files/raw_hist.png\")\n",
    "        \n",
    "        \n",
    "    def visualize_normalized_data(train_x):\n",
    "        # Visualize the normalized images. \n",
    "        print(\"Saving normalized training image ...........\")\n",
    "        plt.imsave('/global/homes/s/ssingh79/Output_files/normalized_x1.png', train_x[0][0])\n",
    "        \n",
    "        # Save histograms of pixels in the normalized image. \n",
    "        fig = plt.figure(figsize=(5, 4))\n",
    "        ax = fig.add_subplot(1,1,1) # one row, one column, first plot\n",
    "        ax.hist((train_x[0][0]), bins=20)\n",
    "        ax.set_title(\"Normalized image pixels Histogram Plot\")\n",
    "        ax.set_xlabel(\"Pixel values\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        print(\"Saving histogram of normalized image.....\") \n",
    "        fig.savefig(\"/global/homes/s/ssingh79/Output_files/norm_hist.png\") \n",
    "    \n",
    "    # Final dataset is here after all the preprocessing. \n",
    "    train_x, valid_x = reshape_data(train_set)\n",
    "    \n",
    "    print(\"Final Training data\", train_x) \n",
    "    print(train_x.shape)\n",
    "    print(\"Final Validation data\", valid_x) \n",
    "    print(valid_x.shape)\n",
    "    \n",
    "    return train_x, valid_x  \n",
    "\n",
    "# ##################### Build the neural network model #######################\n",
    "# This script supports three types of models. For each one, we define a\n",
    "# function that takes a Theano variable representing the input and returns\n",
    "# the output layer of a neural network model built in Lasagne.\n",
    "\n",
    "def build_conv_ae(input_var=None):\n",
    "    \n",
    "    # We will be building a Convolutional Autoencoder with multiple Conv2D and\n",
    "    # Deconv2D layers. The Lasagne layers are packed step by step where output\n",
    "    # of each layer is an input of the next layer. The function returns a \n",
    "    # network ready to be trained. \n",
    "    \n",
    "    conv_num_filter = 64\n",
    "    filter_size = 7\n",
    "    hidden_units = 16*16*64\n",
    "    encode_size = 5000\n",
    "    \n",
    "    # Input layer, specifying the expected input shape of the network\n",
    "    # (unspecified batchsize, 1 channel, 128 rows and 128 columns) and\n",
    "    # linking it to the given Theano variable `input_var`, if any:\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 1, 128, 128),\n",
    "                                     input_var=input_var)\n",
    "    \n",
    "    print(\"Input shape : \", l_in.output_shape )\n",
    "    \n",
    "    #Formula : dim = {(image_size - filter_size + 2*padding)/stride} + 1 \n",
    "    #Formula by convention : alpha =  {(i - k + 2*p)/s} + 1  \n",
    "\n",
    "    # Add First Convolution2D layer -- (64, 64, 64)\n",
    "    l_conv_first = lasagne.layers.Conv2DLayer(l_in, \n",
    "                                              num_filters=conv_num_filter, \n",
    "                                              filter_size=filter_size, \n",
    "                                              stride=(2,2), \n",
    "                                              pad=3)\n",
    "    \n",
    "    print(\"Conv layer 1 : \", l_conv_first.output_shape)\n",
    "    \n",
    "    # Add Second Convolution2D Layer -- (64, 32, 32)\n",
    "    l_conv_second = lasagne.layers.Conv2DLayer(l_conv_first, \n",
    "                                               num_filters=conv_num_filter, \n",
    "                                               filter_size=filter_size,\n",
    "                                               stride=(2,2),\n",
    "                                               pad=3)\n",
    "    \n",
    "    print(\"Conv layer 2 : \", l_conv_second.output_shape)\n",
    "    \n",
    "    \n",
    "    # Add Third Convolution2D Layer -- (64, 16, 16) \n",
    "    l_conv_third = lasagne.layers.Conv2DLayer(l_conv_second, \n",
    "                                               num_filters=conv_num_filter, \n",
    "                                               filter_size=filter_size,\n",
    "                                               stride=(2,2),\n",
    "                                               pad=3)\n",
    "    \n",
    "    print(\"Conv layer 3 : \", l_conv_third.output_shape)\n",
    "    \n",
    "    # Flattened Hidden Layer in the middle\n",
    "    # Use Reshape layer to flatten the images. \n",
    "    #l_reshape_conv = lasagne.layers.ReshapeLayer()\n",
    "    \n",
    "    # Add a fully-connected layer of ###### units, using the non-linear tanh, and\n",
    "    # initializing weights with Glorot's scheme (which is the default anyway):\n",
    "    \n",
    "    l_hid1 = lasagne.layers.DenseLayer(l_conv_third, \n",
    "                                       num_units=hidden_units, \n",
    "                                       nonlinearity=lasagne.nonlinearities.tanh, \n",
    "                                       W=lasagne.init.GlorotUniform()) \n",
    "    \n",
    "    print(\"Dense layer 1 : \", l_hid1.output_shape)\n",
    "    \n",
    "    # We'll now add dropout of 50%:\n",
    "    #l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.2) \n",
    "        \n",
    "    # Encode layer: \n",
    "    #encode = lasagne.layers.DenseLayer(l_hid1_drop,\n",
    "    #                                  num_units=encode_size,\n",
    "    #                                  nonlinearity = lasagne.nonlinearities.tanh,\n",
    "    #                                  W = lasagne.init.GlorotUniform())\n",
    "    \n",
    "    # Decoded fully connected layer. The dimensions are same as the previous\n",
    "    # dense layer before encoding. \n",
    "    #l_hid2 = lasagne.layers.DenseLayer(encode,\n",
    "    #                                  num_units=hidden_units,\n",
    "    #                                  nonlinearity = lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    # Reshape Layer to convert the Fully connected layer into 2D images again.\n",
    "    l_reshape_deconv = lasagne.layers.ReshapeLayer(l_hid1,\n",
    "                                                   shape=(([0], conv_num_filter, 16, 16)))\n",
    "   \n",
    "    print(\"Reshape after Dense Layer : \", l_reshape_deconv.output_shape)\n",
    "    \n",
    "    # Damn! This is so hard-coded!!!\n",
    "    \n",
    "    # Here we start Transposing the Convolutional layers, also called Deconvolution.\n",
    "    # We will be using Lasagne's Deconvolution2D layer. \n",
    "    # Add the first Deconv layer -- (64, 16, 16) \n",
    "    l_deconv_first = lasagne.layers.Deconv2DLayer(l_reshape_deconv,\n",
    "                                                num_filters= conv_num_filter,\n",
    "                                                filter_size = filter_size+1,\n",
    "                                                stride = (2,2),\n",
    "                                                crop=3\n",
    "                                                 )\n",
    "    \n",
    "    print(\"Deconv layer 1 : \", l_deconv_first.output_shape)\n",
    "    \n",
    "    # Add the Second Deconv layer -- (64, 32, 32)\n",
    "    l_deconv_second = lasagne.layers.Deconv2DLayer(l_deconv_first,\n",
    "                                                num_filters= conv_num_filter,\n",
    "                                                filter_size = filter_size+1,\n",
    "                                                stride = (2,2),\n",
    "                                                crop = 3 \n",
    "                                                 )\n",
    "    \n",
    "    print(\"Deconv layer 2 : \", l_deconv_second.output_shape)\n",
    "    \n",
    "    # Add the Third Deconv layer -- (64, 64, 64)\n",
    "    l_deconv_third = lasagne.layers.Deconv2DLayer(l_deconv_second,\n",
    "                                                num_filters= 1,\n",
    "                                                filter_size = filter_size+1,\n",
    "                                                stride = (2,2),\n",
    "                                                crop=3\n",
    "                                                )\n",
    "    \n",
    "    print(\"Final layer Shape : \", l_deconv_third.output_shape)\n",
    "    \n",
    "    print(\"Created Lasagne Layers & Network established !!! \")\n",
    "    \n",
    "    return l_deconv_third     \n",
    "    \n",
    "    \n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "# Notice that this function returns only mini-batches of size `batchsize`.\n",
    "# If the size of the data is not a multiple of `batchsize`, it will not\n",
    "# return the last (remaining) mini-batch.\n",
    "\n",
    "def iterate_minibatches(inputs, batchsize, shuffle=False):\n",
    "    #assert len(inputs) == len(targets)    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt]\n",
    "\n",
    "        \n",
    "# ############################## Main program ################################\n",
    "# Everything else will be handled in our main program now. We could pull out\n",
    "# more functions to better separate the code, but it wouldn't make it any\n",
    "# easier to read.\n",
    "\n",
    "def main(num_epochs = 500):\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    X_train, X_valid = load_data()\n",
    "    \n",
    "    # Prepare Theano variables for inputs\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.tensor4('targets')\n",
    "    \n",
    "    # Create neural network model \n",
    "    print(\"Building model and compiling functions.........\")\n",
    "    network = build_conv_ae(input_var)\n",
    "    \n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.squared_error(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "    \n",
    "    \n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.01, momentum=0.6)\n",
    "    \n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], [loss, updates], updates=updates)\n",
    "    \n",
    "    # Create a theano funtion to get the prediction. \n",
    "    predict_fn = theano.function([input_var], prediction)\n",
    "    \n",
    "    #*****You may have to write multiple theano function to get the Prediction per se\n",
    "    \n",
    "    \n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # Create array to append cost. \n",
    "    cost_plot = []\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        # Assign the batch size here ---\n",
    "        batch_size = 5\n",
    "        start_time = time.time()\n",
    "        #print(X_train.shape)\n",
    "        #print(X_train.shape[0])\n",
    "        \n",
    "        for batch in iterate_minibatches(X_train, batch_size, shuffle=True):\n",
    "            inputs = batch\n",
    "            t_err, updates = train_fn(inputs, inputs)\n",
    "            train_err += t_err\n",
    "            updates\n",
    "            train_batches += 1\n",
    "    \n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "        #print(train_err)\n",
    "        #print(train_batches)\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        \n",
    "        # Append the mean cost in an array for plotting Learning curve. \n",
    "        cost_plot.append(train_err/train_batches)\n",
    "        \n",
    "    \n",
    "    ################### Visualization code ########################\n",
    "    \n",
    "    # Learning curve.  \n",
    "    print(\"Plotting the learning curve .........\")\n",
    "    fig = plt.figure(figsize=(5, 4))\n",
    "    ax = fig.add_subplot(1,1,1) # one row, one column, first plot\n",
    "    ax.plot(cost_plot)\n",
    "    ax.set_title(\"Learning Curve\")\n",
    "    ax.set_xlabel(\"Training loss\")\n",
    "    ax.set_ylabel(\"Epochs\")\n",
    "    fig.savefig(\"/global/homes/s/ssingh79/Output_files/learning_curve.png\")\n",
    "    \n",
    "    # Theano function predict gets the reconstructed output. \n",
    "    print(\"Running prediction function on Validation data\")\n",
    "    pred_images = predict_fn(X_train)\n",
    "    \n",
    "    # Reconstruction of images\n",
    "    print(\"Saving reconstructed images .........\") \n",
    "    plt.imsave('/global/homes/s/ssingh79/Output_files/ae_reconstruct.png', pred_images[0][0]) \n",
    "    \n",
    "\n",
    "    \n",
    "################# Run the code for Conv AE ####################\n",
    "\n",
    "main(500)\n",
    "#load_data()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
