{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import urllib\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "sys.path.insert(0,'/global/common/cori/software/theano/0.8.2/lib/python2.7/site-packages/')\n",
    "import theano\n",
    "sys.path.insert(0,'/global/common/cori/software/lasagne/0.1/lib/python2.7/site-packages/')\n",
    "import lasagne\n",
    "sys.path.insert(0,'/global/common/cori/software/nolearn/0.6/')\n",
    "import nolearn\n",
    "\n",
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from nolearn.lasagne import BatchIterator, PrintLayerInfo\n",
    "from theano.sandbox.neighbours import neibs2images\n",
    "from lasagne.objectives import squared_error\n",
    "from lasagne.nonlinearities import tanh, rectify\n",
    "\n",
    "from shape import ReshapeLayer\n",
    "\n",
    "from IPython.display import Image as IPImage\n",
    "from PIL import Image\n",
    "\n",
    "# Importing modules for creating lasagne layers. \n",
    "from lasagne.layers import get_output, InputLayer, DenseLayer, Deconv2DLayer, Upscale2DLayer \n",
    "from lasagne.layers import Conv2DLayer as Conv2DLayerSlow\n",
    "from lasagne.layers import MaxPool2DLayer as MaxPool2DLayerSlow\n",
    "from lasagne.layers import Deconv2DLayer \n",
    "try:\n",
    "    from lasagne.layers.cuda_convnet import Conv2DCCLayer as Conv2DLayerFast\n",
    "    from lasagne.layers.cuda_convnet import MaxPool2DCCLayer as MaxPool2DLayerFast\n",
    "    print('Using cuda_convnet (faster)')\n",
    "except ImportError:\n",
    "    from lasagne.layers import Conv2DLayer as Conv2DLayerFast\n",
    "    from lasagne.layers import MaxPool2DLayer as MaxPool2DLayerFast\n",
    "    print('Using lasagne.layers (slower)')\n",
    "    \n",
    "#from nolearn.lasagne import TrainSplit\n",
    "\n",
    "#def load_data():\n",
    "\n",
    "    \n",
    "#def build_conv_ae():\n",
    "    \n",
    "    \n",
    "#build_conv_ae()   \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "    \n",
    "# Here we give different data sets for the autoencoders\n",
    "dataurl = '/global/homes/s/ssingh79/data/'\n",
    "#hdf5file = 'conv_z02.h5'\n",
    "hdf5file = 'segment128_data.h5'\n",
    "filepath = os.path.join(dataurl, hdf5file)\n",
    "\n",
    "print(\"Calling \", hdf5file, \"......\")\n",
    "# Call the load_data method to get back the Final training set. \n",
    "dataset = filepath\n",
    "\n",
    "sample_size = 1000\n",
    "\n",
    "with h5py.File(dataset,'r') as hf:\n",
    "    #train_set = hf['X_train'][0:1000,0:65536]\n",
    "    train_set = hf['data_mean_diff_min'][0:1000,:]\n",
    "    print(\"Printing Train set \", train_set)\n",
    "    print(\"X_train shape \", train_set.shape)\n",
    "\n",
    "#Create Training set and Validation set: 80 : 20 Randomly Sampling the images. \n",
    "X = np.random.choice(1000, 1000, replace=False)\n",
    "split_percent = 0.90  \n",
    "\n",
    "#print(X)\n",
    "#Get the random indices of images.  \n",
    "train_split = sample_size*split_percent\n",
    "train_index = X[0:train_split]\n",
    "valid_index = X[train_split:sample_size]\n",
    "\n",
    "X_train = train_set[train_index[:], : ]\n",
    "print(\"Training Set : \", X_train)\n",
    "print(X_train.shape) \n",
    "\n",
    "X_valid = train_set[valid_index[:], : ]\n",
    "print(\"Validation Set : \", X_valid)\n",
    "print(X_valid.shape)\n",
    "\n",
    "#return train_x, valid_x  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train, X_valid = load_data()\n",
    "    \n",
    "# reshape from (sample_size, 128*128) to 4D tensor (sample_size, 1, 128, 128)\n",
    "X_train = np.reshape(X_train, (-1, 1, 128, 128))\n",
    "print('X type and shape:', X_train.dtype, X_train.shape)\n",
    "print('X.min():', X_train.min())\n",
    "print('X.max():', X_train.max())\n",
    "\n",
    "# we need our target to be 1 dimensional\n",
    "X_out = X_train.reshape((X_train.shape[0], -1))\n",
    "print('X_out:', X_out.dtype, X_out.shape)\n",
    "\n",
    "conv_num_filters = 32\n",
    "filter_size = 5\n",
    "pool_size = 2\n",
    "encode_size = 32\n",
    "dense_mid_size = 4096\n",
    "pad_in = 'valid'\n",
    "pad_out = 'full'\n",
    "\n",
    "#Create Lasagne Layers!\n",
    "layers = [\n",
    "    (InputLayer, {'shape': (None, X_train.shape[1], X_train.shape[2], X_train.shape[3])}), \n",
    "    (Conv2DLayerFast, {'num_filters': conv_num_filters, 'filter_size': filter_size, 'pad': pad_in}),\n",
    "    (Conv2DLayerFast, {'num_filters': conv_num_filters, 'filter_size': filter_size, 'pad': pad_in}),\n",
    "    (MaxPool2DLayerFast, {'pool_size': pool_size}),\n",
    "    (Conv2DLayerFast, {'num_filters': 2*conv_num_filters, 'filter_size': filter_size, 'pad': pad_in}),\n",
    "    (MaxPool2DLayerFast, {'pool_size': pool_size}),\n",
    "    (ReshapeLayer, {'shape': (([0], -1))}),\n",
    "    (DenseLayer, {'num_units': dense_mid_size}),\n",
    "    (DenseLayer, {'name': 'encode', 'num_units': encode_size}),\n",
    "    (DenseLayer, {'num_units': dense_mid_size}),\n",
    "    (DenseLayer, {'num_units': 1600}),\n",
    "    (ReshapeLayer, {'shape': (([0], 2*conv_num_filters, 5, 5))}),\n",
    "    (Upscale2DLayer, {'scale_factor': pool_size}),\n",
    "    (Conv2DLayerFast, {'num_filters': conv_num_filters, 'filter_size': filter_size, 'pad': pad_out}),\n",
    "    (Upscale2DLayer, {'scale_factor': pool_size}),\n",
    "    (Conv2DLayerSlow, {'num_filters': conv_num_filters, 'filter_size': filter_size, 'pad': pad_out}),\n",
    "    (Conv2DLayerSlow, {'num_filters': 1, 'filter_size': filter_size, 'pad': pad_out}),\n",
    "    (ReshapeLayer, {'shape': (([0], -1))}),\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create Network \n",
    "ae = NeuralNet(\n",
    "    layers=layers,\n",
    "    max_epochs=50,\n",
    "\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.01,\n",
    "    update_momentum=0.975,\n",
    "\n",
    "\n",
    "    objective_loss_function = squared_error,\n",
    "    verbose=1\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Begin Training. \n",
    "ae.fit(X_train, X_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
