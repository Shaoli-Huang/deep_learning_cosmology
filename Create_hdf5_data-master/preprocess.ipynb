{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The code here preprocesses the Cosmology input data. \n",
    "\n",
    "import numpy as np \n",
    "import h5py\n",
    "from sklearn import preprocessing as pp \n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def read_h5py_dataset():  \n",
    "    with h5py.File('/global/homes/s/ssingh79/data/conv_z02.h5', 'r') as hf:\n",
    "        #print(hf.keys())\n",
    "        print(\"Reading conv_z02.h5 ...........\")\n",
    "        X_train = hf['X_train'][0:1000, 0:1048576]        \n",
    "        print(\"h5py shape \", X_train.shape)\n",
    "        \n",
    "        print(\"Training Data: \", X_train) \n",
    "        return X_train\n",
    "\n",
    "def std_scale(normalize_images):\n",
    "    \n",
    "    ###### Mean-subtraction \n",
    "    std_scale = pp.StandardScaler().fit(normalize_images)\n",
    "    normalized_std_scale = std_scale.transform(normalize_images)\n",
    "    print(\"Standard Scale: \", normalized_std_scale)\n",
    "    \n",
    "    return normalized_std_scale\n",
    "    \n",
    "def min_max_scale(normalize_images):    \n",
    "    \n",
    "    # Normalize the data between [0, 1]. This is called Feature Scaling\n",
    "    min_max_scaler = pp.MinMaxScaler()\n",
    "    normalized_zero_to_one = min_max_scaler.fit_transform(normalize_images)\n",
    "    print(\"Normalize [0,1] : \", normalized_zero_to_one)\n",
    "    \n",
    "    return normalized_zero_to_one\n",
    "    \n",
    "def mean_diff_min_max(normalize_images):\n",
    "    \n",
    "    #Mean difference and between [0,1]\n",
    "    \n",
    "    normalized_std_scale = std_scale(normalize_images)\n",
    "    \n",
    "    min_max_scaler_1 = pp.MinMaxScaler()\n",
    "    normalized_zero_to_one_1 = min_max_scaler_1.fit_transform(normalized_std_scale)\n",
    "    print(\" Standard scale + Normalize [0,1] : \", normalized_zero_to_one_1)\n",
    "    \n",
    "    return normalized_zero_to_one_1\n",
    "    \n",
    "def max_abs_scale(normalize_images):\n",
    "    \n",
    "    # Normalize the data between [-1, 1]. This is called Feature Scaling\n",
    "    max_abs_scaler = pp.MaxAbsScaler()\n",
    "    normalized_neg_pos_one = max_abs_scaler.fit_transform(normalize_images)\n",
    "    print(\"Normalize [-1,1] : \",normalized_neg_pos_one)\n",
    "    \n",
    "    return normalized_neg_pos_one\n",
    "    \n",
    "def mean_diff_max_abs_scale(normalize_images):\n",
    "    \n",
    "    # Normalization with both mean subtraction and between [-1,1] \n",
    "    \n",
    "    normalized_std_scale = std_scale(normalize_images)\n",
    "    \n",
    "    max_abs_scaler_1 = pp.MaxAbsScaler()\n",
    "    normalized_neg_pos_one_1 = max_abs_scaler_1.fit_transform(normalized_std_scale)\n",
    "    print(\"Standard Scale + Normalize [-1,1] : \",normalized_neg_pos_one_1)\n",
    "    \n",
    "    return normalized_neg_pos_one_1\n",
    "\n",
    "def create_h5py_dataset(normalize_images):\n",
    "        \n",
    "    # Call various functions and create h5py data files from returned numpy   \n",
    "    #data_std_scale = std_scale(normalize_images)    \n",
    "    #data_min_max = min_max_scale(normalize_images)\n",
    "    data_mean_diff_min = mean_diff_min_max(normalize_images)\n",
    "    #data_max_abs = max_abs_scale(normalize_images)\n",
    "    #data_mean_diff_abs = mean_diff_max_abs_scale(normalize_images)\n",
    "    \n",
    "    # Create h5py data files with 5 techniques \n",
    "    with h5py.File('/global/homes/s/ssingh79/data/normalized_data.h5','w') as hf:\n",
    "        # X_train is the training set needed for unsupervised learning. \n",
    "       \n",
    "        print(\"Creating h5py data files and saving to ./data/normalized_data.h5 .........\")\n",
    "        #hf.create_dataset('data_std_scale', data = data_std_scale[0:10,:])\n",
    "        #hf.create_dataset('data_min_max', data = data_min_max[0:10,:])\n",
    "        hf.create_dataset('data_mean_diff_min', data = data_mean_diff_min[0:10,:]) \n",
    "        #hf.create_dataset('data_max_abs', data = data_max_abs[0:10,:])\n",
    "        #hf.create_dataset('data_mean_diff_abs', data = data_mean_diff_abs[0:10,:])\n",
    "        \n",
    "    \n",
    "def normalize_data():\n",
    "    # This normalizes the data between [-1, 1]\n",
    "    # A two step process which includes calculating the mean difference and then\n",
    "    # normalizing by known standard formulae, here MaxAbsScaler to get data \n",
    "    # between [-1, 1].\n",
    "    \n",
    "    normalize_images = read_h5py_dataset()\n",
    "    \n",
    "    # Get the Mean, Minimum and Maximum values from the dataset \n",
    "    print(\"Mean = \", np.mean(normalize_images))\n",
    "    print(\"Min = \", np.min(normalize_images))\n",
    "    print(\"Max = \", np.max(normalize_images))\n",
    "    \n",
    "    create_h5py_dataset(normalize_images)\n",
    "\n",
    "def read_normalized_data():\n",
    "    \n",
    "    # Show the original image\n",
    "    with h5py.File('/global/homes/s/ssingh79/data/conv_z02.h5', 'r') as hr:\n",
    "        org = hr['X_train'][0,:]\n",
    "        org = np.reshape(org, (1024,1024))\n",
    "        \n",
    "        #%matplotlib inline\n",
    "        #plt.imshow(org, interpolation='None')\n",
    "        #plt.colorbar()\n",
    "    \n",
    "    # Read the normalized hdf5 data here\n",
    "    with h5py.File('/global/homes/s/ssingh79/data/normalized_data.h5','r') as hf:\n",
    "        print(hf.keys())\n",
    "        #image1 = hf['data_std_scale'][:,:]\n",
    "        #image2 = hf['data_min_max'][:,:]\n",
    "        image3 = hf['data_mean_diff_min'][:,:]\n",
    "        #image4 = hf['data_max_abs'][:,:]\n",
    "        #image5 = hf['data_mean_diff_abs'][:,:]\n",
    "        \n",
    "        #print(\"Standard Scaling i.e Zero mean and unit co-variance: \",image1)\n",
    "        #print(\"Normalize between [0,1]\", image2)\n",
    "        print(\"Standard scaling and normalize between [0,1]\", image3)\n",
    "        #print(\"Normalize between [-1,1]\", image4)\n",
    "        #print(\"Standard scaling and normalize between [-1,1]\", image5)\n",
    "        print(image3.shape)\n",
    "        \n",
    "        # Here we reshape the matrix to get images back again to 1024 X 1024 from flattened array. \n",
    "        # If you just change the row index from 0 to n=10. You can see all 10 set of images. \n",
    "        #print_image1 = np.reshape(image1[0,:],(1024,1024))\n",
    "        #print_image2 = np.reshape(image2[0,:],(1024,1024))\n",
    "        print_image3 = np.reshape(image3[0,:],(1024,1024))\n",
    "        #print_image4 = np.reshape(image4[0,:],(1024,1024)) \n",
    "        #print_image5 = np.reshape(image5[0,:],(1024,1024)) \n",
    "        \n",
    "        #%matplotlib inline\n",
    "        #plt.imshow(print_image1, interpolation='None')\n",
    "        #plt.colorbar()\n",
    "        '''\n",
    "        %matplotlib inline\n",
    "        plt.subplot(321)\n",
    "        plt.imshow(print_image1, cmap='Greys_r')\n",
    "        plt.subplot(322)\n",
    "        plt.imshow(print_image2, cmap='Greys_r')\n",
    "        plt.subplot(323)\n",
    "        plt.imshow(print_image3, cmap='Greys_r')\n",
    "        plt.subplot(324)\n",
    "        plt.imshow(print_image4, cmap='Greys_r')\n",
    "        plt.subplot(325)\n",
    "        plt.imshow(print_image4, cmap='Greys_r')\n",
    "        plt.colorbar()        \n",
    "    '''\n",
    "    \n",
    "#Run command to preprocess the data.\n",
    "normalize_data()\n",
    "\n",
    "#Run the command to read preprocessed data. Comment above. \n",
    "#read_normalized_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.62007241  0.18555646  0.96371445 ...,  0.2516714   0.34155526\n",
      "   0.59846914]\n",
      " [ 0.08337607  0.52154995  0.68974707 ...,  0.03682898  0.97789935\n",
      "   0.16802598]\n",
      " [ 0.86050826  0.01293113  0.5142857  ...,  0.9461386   0.57028538\n",
      "   0.63293017]\n",
      " ..., \n",
      " [ 0.08567538  0.64891737  0.53060225 ...,  0.14180843  0.98321152\n",
      "   0.65700966]\n",
      " [ 0.52214182  0.4298153   0.69926911 ...,  0.93296286  0.32569724\n",
      "   0.76974693]\n",
      " [ 0.57710629  0.50496848  0.24516108 ...,  0.38527116  0.16196241\n",
      "   0.63914528]]\n",
      "-------------Standard Scaler-------------------------------------\n",
      "[[ 0.39261521 -1.13572799  1.60322648 ..., -0.93020077 -0.57860904\n",
      "   0.36404606]\n",
      " [-1.41672785  0.05490987  0.63348203 ..., -1.67915534  1.59743231\n",
      "  -1.15358735]\n",
      " [ 1.20318704 -1.74744894  0.01241285 ...,  1.49075687  0.20355616\n",
      "   0.4855469 ]\n",
      " ..., \n",
      " [-1.4089763   0.50625338  0.07016749 ..., -1.3131902   1.6155978\n",
      "   0.57044509]\n",
      " [ 0.0624657  -0.27016413  0.66718654 ...,  1.44482537 -0.63283711\n",
      "   0.96792811]\n",
      " [ 0.24776521 -0.00384878 -0.94019    ..., -0.46446334 -1.19274449\n",
      "   0.50745981]]\n",
      "5.91171556152e-18\n",
      "-----------MinMAxScaler------------------------------------------\n",
      "[[ 0.62052082  0.1840831   0.96402562 ...,  0.2471136   0.34156767\n",
      "   0.59826851]\n",
      " [ 0.08318953  0.52117998  0.68992086 ...,  0.03086915  0.97798881\n",
      "   0.16619695]\n",
      " [ 0.86124113  0.01089087  0.5143715  ...,  0.94611282  0.57032548\n",
      "   0.63285991]\n",
      " ..., \n",
      " [ 0.08549155  0.64896568  0.53069623 ...,  0.13653368  0.98330162\n",
      "   0.65703049]\n",
      " [ 0.52247438  0.42914407  0.69944767 ...,  0.9328511   0.32570773\n",
      "   0.77019426]\n",
      " [ 0.57750386  0.50454406  0.24511192 ...,  0.38158522  0.16195308\n",
      "   0.63909853]]\n",
      "0.0\n",
      "1.0\n",
      "--------------MaxAbsScaler---------------------------------------\n",
      "[[ 0.23137885 -0.63593588  0.88705342 ..., -0.52080884 -0.33129786\n",
      "   0.20508352]\n",
      " [-0.83491637  0.03074606  0.35050095 ..., -0.94013999  0.91465197\n",
      "  -0.64986764]\n",
      " [ 0.70907095 -0.97846095  0.00686794 ...,  0.83465783  0.11655144\n",
      "   0.27353041]\n",
      " ..., \n",
      " [-0.83034818  0.28346989  0.03882315 ..., -0.73524027  0.9250531\n",
      "   0.32135738]\n",
      " [ 0.03681274 -0.15127484  0.36914941 ...,  0.80894131 -0.36234757\n",
      "   0.54527744]\n",
      " [ 0.1460148  -0.00215508 -0.52020021 ..., -0.26004775 -0.6829373\n",
      "   0.28587493]]\n",
      "0.0\n",
      "1.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing as pp \n",
    "import numpy as np\n",
    "\n",
    "A = np.random.random((1000,1000))\n",
    "print(A)\n",
    "\n",
    "print(\"-------------Standard Scaler-------------------------------------\")\n",
    "\n",
    "std_scale = pp.StandardScaler().fit(A)\n",
    "X_train = std_scale.transform(A)\n",
    "print(X_train)\n",
    "print(np.mean(X_train))\n",
    "\n",
    "print(\"-----------MinMAxScaler------------------------------------------\")\n",
    "\n",
    "\n",
    "min_max_scaler = pp.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "print(X_train_minmax)\n",
    "print(np.min(X_train_minmax))\n",
    "print(np.max(X_train_minmax))\n",
    "\n",
    "print(\"--------------MaxAbsScaler---------------------------------------\")\n",
    "\n",
    "max_abs_scaler = pp.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "print(X_train_maxabs)\n",
    "print(np.min(X_train_minmax))\n",
    "print(np.max(X_train_minmax))\n",
    "\n",
    "\n",
    "\n",
    "print(\"------------------------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'lasagne'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-950fc8a00ce6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'lasagne'"
     ]
    }
   ],
   "source": [
    "import lasagne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'nolearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b41976294198>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnolearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'nolearn'"
     ]
    }
   ],
   "source": [
    "import nolearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
